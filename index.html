<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
<title>PropMiner</title>
<meta charset="UTF-8">
<link rel="stylesheet" href="styles/layout.css" type="text/css">
<!--[if lt IE 9]><script src="scripts/html5shiv.js"></script><![endif]-->
</head>
<body>
<div class="wrapper row1">
  <header id="header" class="clear">
    <div id="hgroup" style="text-align:center;">
      <h1><b>PropMiner: Enhanced Code and Comment Representation for Code Search and Code Summarization</b></h1>
    </div>
    <nav>
      <ul>
        <li><a style="color:#FF9900; background-color:#FFFFFF;" href="index.html">Introduction</a></li>
        <li><a href="approach.html">Approach</a></li>
        <li><a href="demoCodeSearch.html">Demo - Code Search</a></li>
        <li><a href="demoCodeSum.html">Demo - Code Summarization</a></li>
        <!-- <li><a href="evaluation.html">Evaluation</a></li> -->
      </ul>
    </nav>
  </header>
</div>
<!-- content -->
<div class="wrapper row2">
  <div id="container" class="clear">
    <section id="slider">
    <p>The graph representation for source code has been applied in many research works in Software Engineering. In the problem of code-to-comment and comment-to-code inference, pre-trained models for programming languages like GraphCodeBERT used data flow graphs for code representation. While this semantic structure representation of code is considered as an input, information about the syntactic-level structure of the code such as the Abstract Syntax Tree has been neglected in GraphCodeBERT. Similarly, this language model for source code has not taken advantage of the tree representation of code comments such as the natural language parse tree. In this work, we analyze and filter meaningful quantity properties in a dataset consisting of natural language parse trees and abstract syntax trees. We propose PropMiner, a Recurrent Neural Network-based approach for mining natural language parse trees’ properties from source code representation and mining abstract syntax trees’ properties from comment representation. Additionally, we provide an algorithm to augment our mined features for the pre-trained embedding of GraphCodeBERT. With the evaluation of 7583 pairs of code comment-implementation of the TLCodeSum dataset, we show that the new embedding, as the combination of GraphCodeBERT and PropMiner, achieves the Mean Reciprocal Rank as (60.8%, 76.4%) for code search and code summarization,respectively, which outperforms the original GraphCodeBERT by 1.5% and 1.7%. PropMiner can also improve the Mean Reciprocal Rank of the natural language model FastText in the same two downstream tasks from (41.8%, 41.4%) to (44.7%, 52.3%), which shows the effectiveness of the new representation.</p>
    <p>The replication package of PropMiner is avaiable at <a href="https://tinyurl.com/mr3bj3vh">this site</a>.</p>
    </section>
  </div>
</div>
<!-- footer -->
<div class="wrapper row3">
  <footer id="footer" class="clear">
    <p class="fl_left">Copyright &copy; 2018 - All Rights Reserved - <a href="#">Domain Name</a></p>
    <p class="fl_right">Template by <a target="_blank" href="https://www.os-templates.com/" title="Free Website Templates">OS Templates</a></p>
  </footer>
</div>
</body>
</html>
